{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "# EDIT\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.143:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.143:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2167099d8a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text (text) :\n",
    "    words_List = nltk.word_tokenize(text)\n",
    "    final_list = [elto for elto in words_List if elto not in STOP_WORDS]\n",
    "    return \" \".join(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Include the username in the text to increase classification accuracy\n",
    "    # From tests we can see that adding it before or after processing the text data doesn't matter\n",
    "    df.loc[:, 'message'] = df['message'] + ' ' + df['username']\n",
    "\n",
    "    # Notice that we want Sleep = SLEEP = SlEEp = sleeP ETC   \n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].str.lower()\n",
    "\n",
    "    # Drop NaN values\n",
    "    df.dropna(inplace=True, subset=['channel', 'message'])\n",
    "\n",
    "    # Remove words like: can, could, will, been, would...\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].apply(clean_text)\n",
    "\n",
    "    # stem separate words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].astype(str).str.split()\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Remove rows with empty messages\n",
    "    df = df[df['message'].astype(bool)]\n",
    "\n",
    "    # Rejoin list of messages to single string message separated by <space>\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    df.rename(columns={'channel': 'label'}, inplace=True)\n",
    "\n",
    "    final_df = df.loc[:, ['message', 'label']]\n",
    "\n",
    "    # print(final_df)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = 'models\\\\'\n",
    "MODEL = 'multinomialNB'\n",
    "STOP = text.ENGLISH_STOP_WORDS\n",
    "STOP_WORDS = list(STOP) + list(string.punctuation)\n",
    "\n",
    "# Create a list of predictions to concat all predictions and later save them into .csv format\n",
    "# for further processing\n",
    "predictions_list = []\n",
    "\n",
    "mapping = {0:'#loltyler1', 1:'#gothamchess'}\n",
    "\n",
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "globals()['my_model'] = NaiveBayesModel.load(MODELS_PATH+MODEL)\n",
    "globals()['models_loaded'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()['models_loaded'] = False\n",
    "# globals()['my_model'] = None\n",
    "\n",
    "# Toy predict function. Normally you'd use your loaded globals()['my_model'] here\n",
    "# def predict(df):\n",
    "#     df.show()\n",
    "#     print(globals()['my_model'])\n",
    "#     predictions = globals()['my_model'].transform(df)\n",
    "#     # predictions = predictions.withColumn('prediction', \n",
    "#     #                     when(col('prediction') == 0, lit(mapping[0])).otherwise(lit(mapping[1])))\n",
    "    \n",
    "#     predictions.show()\n",
    "#     return predictions.prediction\n",
    "\n",
    "# predict_udf = udf(predict, StringType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to PySpark DataFrame\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    # Convert to Pandas DataFrame for preprocessing\n",
    "    df_pandas = df.toPandas()\n",
    "    df_pandas = preprocess(df_pandas)\n",
    "    # Reconvert to PySpark DataFrame (I'm sure there is a better way to do this)\n",
    "    df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # break the sentence into a list of words\n",
    "    tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "    words_data = tokenizer.transform(df)\n",
    "\n",
    "    # TF section\n",
    "    hashing_TF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=200000)\n",
    "    featurized_data = hashing_TF.transform(words_data)\n",
    "\n",
    "    # IDF section\n",
    "    idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "    idf_model = idf.fit(featurized_data)\n",
    "\n",
    "    rescaled_data = idf_model.transform(featurized_data)\n",
    "    \n",
    "    # rescaled_data.show()\n",
    "\n",
    "    # # Utilize our predict function\n",
    "    # df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "    #     struct([df[x] for x in df.columns])\n",
    "    # ))\n",
    "    # df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
    "    # but an MLlib model you've built and saved with Spark\n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = NaiveBayesModel.load(MODELS_PATH+MODEL)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(rescaled_data)\n",
    "    df_result = df_result.withColumn('prediction', \n",
    "        when(col('prediction') == 0, lit(mapping[0])).otherwise(lit(mapping[1])))\n",
    "\n",
    "\n",
    "    to_concat = df_result.toPandas()\n",
    "    predictions_list.append(to_concat)\n",
    "\n",
    "    try:\n",
    "        df_result.select(['message', 'label', 'probability', 'prediction']).show()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        # Uncomment to see what went wrong\n",
    "        # print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel: uid=NaiveBayes_b32a0b916ba2, modelType=multinomial, numClasses=2, numFeatures=200000\n"
     ]
    }
   ],
   "source": [
    "print(globals()['my_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-23 19:22:20 =========\n",
      "Success\n",
      "+--------------------+----------+--------------------+------------+\n",
      "|             message|     label|         probability|  prediction|\n",
      "+--------------------+----------+--------------------+------------+\n",
      "|   kekw lol darwangg|#loltyler1|[0.41396012296278...|#gothamchess|\n",
      "|  kekw toddygotbeatz|#loltyler1|[0.99999819794434...|  #loltyler1|\n",
      "|   kek snowmanofpoop|#loltyler1|[0.90440650882311...|  #loltyler1|\n",
      "|jebait jebait jeb...|#loltyler1|[0.57274347345359...|  #loltyler1|\n",
      "|jebait cherry_buiiet|#loltyler1|[0.54130153870317...|  #loltyler1|\n",
      "|  rank ianskilamang1|#loltyler1|[0.99999993893297...|  #loltyler1|\n",
      "|        kekw cap2567|#loltyler1|[0.99999984103421...|  #loltyler1|\n",
      "|      kekw raptusrit|#loltyler1|[0.73050045815373...|  #loltyler1|\n",
      "|      kekw nyekon_11|#loltyler1|[0.73050045815373...|  #loltyler1|\n",
      "|ianskilamang1 acc...|#loltyler1|[1.0,1.8046667086...|  #loltyler1|\n",
      "|    jebait tobyraven|#loltyler1|[0.99996263729261...|  #loltyler1|\n",
      "|  kekw learning2read|#loltyler1|[0.99999581292994...|  #loltyler1|\n",
      "|pepega closetmonk...|#loltyler1|[0.99999860733314...|  #loltyler1|\n",
      "|      tf noteriedts1|#loltyler1|[0.97029868141406...|  #loltyler1|\n",
      "|   kekw king_omar_65|#loltyler1|[0.73050045815373...|  #loltyler1|\n",
      "|best lucain iokyr...|#loltyler1|[0.20065822077159...|#gothamchess|\n",
      "|       rank ziomal02|#loltyler1|[0.99999993993743...|  #loltyler1|\n",
      "|  pepelaugh thorudan|#loltyler1|[0.99741444770716...|  #loltyler1|\n",
      "+--------------------+----------+--------------------+------------+\n",
      "\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(predictions_list, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
