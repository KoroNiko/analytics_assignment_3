{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "# EDIT\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.143:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.143:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24cbcae57b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text (text) :\n",
    "    words_List = nltk.word_tokenize(text)\n",
    "    final_list = [elto for elto in words_List if elto not in STOP_WORDS]\n",
    "    return \" \".join(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Include the username in the text to increase classification accuracy\n",
    "    # From tests we can see that adding it before or after processing the text data doesn't matter\n",
    "    df.loc[:, 'message'] = df['message'] + ' ' + df['username']\n",
    "\n",
    "    # Notice that we want Sleep = SLEEP = SlEEp = sleeP ETC   \n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].str.lower()\n",
    "\n",
    "    # Drop NaN values\n",
    "    df.dropna(inplace=True, subset=['channel', 'message'])\n",
    "\n",
    "    # Remove words like: can, could, will, been, would...\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].apply(clean_text)\n",
    "\n",
    "    # stem separate words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].astype(str).str.split()\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Remove rows with empty messages\n",
    "    df = df[df['message'].astype(bool)]\n",
    "\n",
    "    # Rejoin list of messages to single string message separated by <space>\n",
    "    df.loc[:, 'message'] = df.loc[:, 'message'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    df.rename(columns={'channel': 'label'}, inplace=True)\n",
    "\n",
    "    final_df = df.loc[:, ['message', 'label']]\n",
    "\n",
    "    print(final_df)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = 'models\\\\'\n",
    "MODEL = 'multinomialNB'\n",
    "STOP = text.ENGLISH_STOP_WORDS\n",
    "STOP_WORDS = list(STOP) + list(string.punctuation)\n",
    "\n",
    "mapping = {0:'#loltyler1', 1:'#gothamchess'}\n",
    "\n",
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "globals()['my_model'] = NaiveBayesModel.load(MODELS_PATH+MODEL)\n",
    "globals()['models_loaded'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()['models_loaded'] = False\n",
    "# globals()['my_model'] = None\n",
    "\n",
    "# Toy predict function. Normally you'd use your loaded globals()['my_model'] here\n",
    "# def predict(df):\n",
    "#     df.show()\n",
    "#     print(globals()['my_model'])\n",
    "#     predictions = globals()['my_model'].transform(df)\n",
    "#     # predictions = predictions.withColumn('prediction', \n",
    "#     #                     when(col('prediction') == 0, lit(mapping[0])).otherwise(lit(mapping[1])))\n",
    "    \n",
    "#     predictions.show()\n",
    "#     return predictions.prediction\n",
    "\n",
    "# predict_udf = udf(predict, StringType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to PySpark DataFrame\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    # Convert to Pandas DataFrame for preprocessing\n",
    "    df_pandas = df.toPandas()\n",
    "    df_pandas = preprocess(df_pandas)\n",
    "    # Reconvert to PySpark DataFrame (I'm sure there is a better way to do this)\n",
    "    df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # break the sentence into a list of words\n",
    "    tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "    words_data = tokenizer.transform(df)\n",
    "\n",
    "    # TF section\n",
    "    hashing_TF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=200000)\n",
    "    featurized_data = hashing_TF.transform(words_data)\n",
    "\n",
    "    # IDF section\n",
    "    idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "    idf_model = idf.fit(featurized_data)\n",
    "\n",
    "    rescaled_data = idf_model.transform(featurized_data)\n",
    "    \n",
    "    rescaled_data.show()\n",
    "\n",
    "    # # Utilize our predict function\n",
    "    # df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "    #     struct([df[x] for x in df.columns])\n",
    "    # ))\n",
    "    # df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
    "    # but an MLlib model you've built and saved with Spark\n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = NaiveBayesModel.load(MODELS_PATH+MODEL)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(rescaled_data)\n",
    "    df_result = df_result.withColumn('prediction', \n",
    "        when(col('prediction') == 0, lit(mapping[0])).otherwise(lit(mapping[1])))\n",
    "\n",
    "    try:\n",
    "        df_result.select(['message', 'label', 'probability', 'prediction']).show()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        # Uncomment to see what went wrong\n",
    "        # print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel: uid=NaiveBayes_b32a0b916ba2, modelType=multinomial, numClasses=2, numFeatures=200000\n"
     ]
    }
   ],
   "source": [
    "print(globals()['my_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "# ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-23 02:50:20 =========\n",
      "                                             message       label\n",
      "0                                        op.gg risqi  #loltyler1\n",
      "1  risqi https //www.op.gg/summoners/kr/big 20ton...  #loltyler1\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|             message|     label|               words|         rawFeatures|            features|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|         op.gg risqi|#loltyler1|      [op.gg, risqi]|(200000,[18236,41...|(200000,[18236,41...|\n",
      "|risqi https //www...|#loltyler1|[risqi, https, //...|(200000,[41575,55...|(200000,[41575,55...|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "\n",
      "Checkpoint 1\n",
      "Checkpoint 2\n",
      "Checkpoint 3\n",
      "Checkpoint 4\n",
      "DataFrame[message: string, label: string, words: array<string>, rawFeatures: vector, features: vector, rawPrediction: vector, probability: vector, prediction: string]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nikos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Nikos\\AppData\\Local\\Temp\\ipykernel_13616\\265281298.py\", line 11, in run\n",
      "  File \"C:\\Users\\Nikos\\Desktop\\analytics_project\\analytics_assignment_3\\spark-3.2.1-bin-hadoop2.7\\python\\pyspark\\streaming\\context.py\", line 200, in awaitTermination\n",
      "    self._jssc.awaitTermination()\n",
      "  File \"C:\\Users\\Nikos\\Desktop\\analytics_project\\analytics_assignment_3\\spark-3.2.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\", line 1321, in __call__\n",
      "  File \"C:\\Users\\Nikos\\Desktop\\analytics_project\\analytics_assignment_3\\spark-3.2.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\Nikos\\Desktop\\analytics_project\\analytics_assignment_3\\spark-3.2.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o560.awaitTermination.\n",
      ": org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nikos\\Desktop\\analytics_project\\analytics_assignment_3\\spark-3.2.1-bin-hadoop2.7\\python\\pyspark\\streaming\\util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"C:\\Users\\Nikos\\AppData\\Local\\Temp\\ipykernel_13616\\2771826922.py\", line 81, in process\n",
      "    print(df_result.head(1).isEmpty)\n",
      "AttributeError: 'list' object has no attribute 'isEmpty'\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
