{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.143:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.143:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x210e120ef20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "# Toy predict function. Normally you'd use your loaded globals()['my_model'] here\n",
    "def predict(df):\n",
    "    return 'predicted-name-of-channel'\n",
    "\n",
    "predict_udf = udf(predict, StringType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    print(type(df))\n",
    "    # Utilize our predict function\n",
    "    df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "        struct([df[x] for x in df.columns])\n",
    "    ))\n",
    "    df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
    "    # but an MLlib model you've built and saved with Spark\n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = '***' # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    # df_result = globals()['my_model'].transform(df)\n",
    "    # df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:16:00 =========\n",
      "+-------+--------------------+--------------------+--------+\n",
      "|channel|            datetime|             message|username|\n",
      "+-------+--------------------+--------------------+--------+\n",
      "|   #pgl|2022-05-20T22:15:...|jabbi looks like ...|   sgaag|\n",
      "+-------+--------------------+--------------------+--------+\n",
      "\n",
      "+-------+--------------------+--------------------+--------+--------------------+\n",
      "|channel|            datetime|             message|username|                pred|\n",
      "+-------+--------------------+--------------------+--------+--------------------+\n",
      "|   #pgl|2022-05-20T22:15:...|jabbi looks like ...|   sgaag|predicted-name-of...|\n",
      "+-------+--------------------+--------------------+--------+--------------------+\n",
      "\n",
      "========= 2022-05-21 00:16:00 =========\n",
      "+-------+--------------------+--------------------+--------+\n",
      "|channel|            datetime|             message|username|\n",
      "+-------+--------------------+--------------------+--------+\n",
      "|   #pgl|2022-05-20T22:15:...|jabbi looks like ...|   sgaag|\n",
      "+-------+--------------------+--------------------+--------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-------+--------------------+--------------------+--------+--------------------+\n",
      "|channel|            datetime|             message|username|                pred|\n",
      "+-------+--------------------+--------------------+--------+--------------------+\n",
      "|   #pgl|2022-05-20T22:15:...|jabbi looks like ...|   sgaag|predicted-name-of...|\n",
      "+-------+--------------------+--------------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "========= 2022-05-21 00:16:10 =========\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "|   channel|            datetime|             message|     username|\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "|#iitztimmy|2022-05-20T22:15:...|BatChest offline ...|       fengoe|\n",
      "|      #pgl|2022-05-20T22:15:...|My future one nig...|skinnysicario|\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "\n",
      "+----------+--------------------+--------------------+-------------+--------------------+\n",
      "|   channel|            datetime|             message|     username|                pred|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+\n",
      "|#iitztimmy|2022-05-20T22:15:...|BatChest offline ...|       fengoe|predicted-name-of...|\n",
      "|      #pgl|2022-05-20T22:15:...|My future one nig...|skinnysicario|predicted-name-of...|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+\n",
      "\n",
      "========= 2022-05-21 00:16:10 =========\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "|   channel|            datetime|             message|     username|\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "|#iitztimmy|2022-05-20T22:15:...|BatChest offline ...|       fengoe|\n",
      "|      #pgl|2022-05-20T22:15:...|My future one nig...|skinnysicario|\n",
      "+----------+--------------------+--------------------+-------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+--------------------+--------------------+-------------+--------------------+\n",
      "|   channel|            datetime|             message|     username|                pred|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+\n",
      "|#iitztimmy|2022-05-20T22:15:...|BatChest offline ...|       fengoe|predicted-name-of...|\n",
      "|      #pgl|2022-05-20T22:15:...|My future one nig...|skinnysicario|predicted-name-of...|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "print('completed')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ef7bfce4295b9231ba342c7940904953e4109385f03de18d3a38947780f5dd0"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
